[
  {
    "objectID": "workshops.html",
    "href": "workshops.html",
    "title": "Workshops Attended",
    "section": "",
    "text": "2024\n\nAdvanced Tools for Literate Programming and Machine Learning, August 6-7, 2024"
  },
  {
    "objectID": "posts/post_with_code.html",
    "href": "posts/post_with_code.html",
    "title": "A sample work",
    "section": "",
    "text": "For a demonstration of a line plot on a polar axis, see Figure¬†1.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nFigure¬†1: A line plot on a polar axis"
  },
  {
    "objectID": "posts/another_post.html",
    "href": "posts/another_post.html",
    "title": "Blogpost Example",
    "section": "",
    "text": "Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Support Vector Machines Through the Four Lenses",
    "section": "",
    "text": "Support Vector Machines (SVMs) are among the most powerful and mathematically elegant machine learning algorithms.\nBy examining SVM through the four foundational pillars‚ÄîLinear Algebra, Statistics, Calculus, and Optimization‚Äîwe uncover the deep mathematical structure that enables robust and generalizable learning.\nThis deconstruction demonstrates that even advanced AI systems are built upon a small set of fundamental mathematical principles."
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "Support Vector Machines Through the Four Lenses",
    "section": "",
    "text": "Support Vector Machines (SVMs) are among the most powerful and mathematically elegant machine learning algorithms.\nBy examining SVM through the four foundational pillars‚ÄîLinear Algebra, Statistics, Calculus, and Optimization‚Äîwe uncover the deep mathematical structure that enables robust and generalizable learning.\nThis deconstruction demonstrates that even advanced AI systems are built upon a small set of fundamental mathematical principles."
  },
  {
    "objectID": "index.html#the-linear-algebra-perspective",
    "href": "index.html#the-linear-algebra-perspective",
    "title": "Support Vector Machines Through the Four Lenses",
    "section": "0.2 The Linear Algebra Perspective",
    "text": "0.2 The Linear Algebra Perspective\n\n0.2.1 Geometry of Separation\nFrom a linear algebra viewpoint, Support Vector Machines treat data as points in a high-dimensional vector space. Each input sample is represented as a feature vector, transforming learning into a problem of geometric separation.\nAt the core of SVM lies the concept of a hyperplane, defined by a weight vector and a bias term. This hyperplane acts as a decision boundary that separates data points belonging to different classes. Linear algebra governs how this boundary is oriented and positioned through operations such as dot products, vector norms, and projections.\nA key innovation of SVM is the margin‚Äîthe distance between the hyperplane and the nearest data points from each class. These nearest points, known as support vectors, uniquely define the model. As a result, SVM decisions depend on a small subset of critical data points rather than the entire dataset.\nWhen data is not linearly separable, SVMs employ kernel functions that implicitly map data into higher-dimensional spaces, where linear separation becomes feasible. This highlights the expressive power of linear algebra in modern machine learning."
  },
  {
    "objectID": "index.html#the-statistical-foundation",
    "href": "index.html#the-statistical-foundation",
    "title": "Support Vector Machines Through the Four Lenses",
    "section": "0.3 The Statistical Foundation",
    "text": "0.3 The Statistical Foundation\n\n0.3.1 Risk, Margin, and Generalization\nStatistics provides the framework for understanding what makes an SVM model effective. Unlike regression models that minimize squared error, SVMs rely on the hinge loss, which penalizes incorrect classifications and points that lie too close to the decision boundary.\nA central statistical concept in SVM is regularization, which balances two competing objectives:\n\nMaximizing the margin (model simplicity)\nMinimizing classification errors (data fit)\n\nThis balance reflects the classical bias‚Äìvariance trade-off. By controlling the regularization parameter, SVMs reduce overfitting while maintaining strong predictive performance.\nRather than modeling class probabilities explicitly, SVMs are grounded in statistical learning theory and the principle of structural risk minimization, ensuring robustness even in high-dimensional feature spaces."
  },
  {
    "objectID": "index.html#the-calculus-engine",
    "href": "index.html#the-calculus-engine",
    "title": "Support Vector Machines Through the Four Lenses",
    "section": "0.4 The Calculus Engine",
    "text": "0.4 The Calculus Engine\n\n0.4.1 Gradients and Constraints\nAlthough often presented geometrically, SVM training relies heavily on calculus. The learning objective is formulated as a constrained optimization problem, combining margin maximization with penalty terms for misclassification.\nCalculus enables this formulation through:\n\nGradients, which describe how the objective function changes\nLagrange multipliers, which incorporate constraints directly into the optimization process\n\nThese tools transform intuitive geometric goals into a mathematically solvable problem. Even when closed-form solvers or quadratic programming techniques are used, calculus remains the underlying engine that guarantees correctness and convergence."
  },
  {
    "objectID": "index.html#the-optimization-strategy",
    "href": "index.html#the-optimization-strategy",
    "title": "Support Vector Machines Through the Four Lenses",
    "section": "0.5 The Optimization Strategy",
    "text": "0.5 The Optimization Strategy\n\n0.5.1 Finding the Maximum-Margin Solution\nOptimization unifies all previous pillars into a practical learning algorithm. The SVM objective function is convex, meaning it has a single global minimum‚Äîan essential property for stable and reliable learning.\nOptimization theory governs:\n\nHow constraints are enforced\nWhy convergence is guaranteed\nHow efficiently solutions are found\n\nModern SVM implementations rely on quadratic programming and dual optimization methods, enabling efficient training even for large-scale datasets. The final model is defined entirely by its support vectors, making SVM both computationally efficient and interpretable."
  },
  {
    "objectID": "index.html#mapping-svm-to-the-four-pillars",
    "href": "index.html#mapping-svm-to-the-four-pillars",
    "title": "Support Vector Machines Through the Four Lenses",
    "section": "0.6 Mapping SVM to the Four Pillars",
    "text": "0.6 Mapping SVM to the Four Pillars\nSupport Vector Machines demonstrate a seamless integration of all four foundational pillars:\n\nLinear Algebra: Feature spaces, hyperplanes, margins, kernels\n\nStatistics: Generalization, regularization, risk minimization\n\nCalculus: Gradients, constrained optimization, Lagrangians\n\nOptimization: Convex solvers, convergence, efficiency\n\nThis unified view reveals SVM as a mathematically disciplined system rather than a black-box classifier."
  },
  {
    "objectID": "index.html#curriculum-alignment",
    "href": "index.html#curriculum-alignment",
    "title": "Support Vector Machines Through the Four Lenses",
    "section": "0.7 Curriculum Alignment",
    "text": "0.7 Curriculum Alignment\n\n0.7.1 Core Machine Learning\nSupport Vector Machines act as a bridge between simple linear models and advanced classifiers, drawing equally from all four pillars.\n\n\n0.7.2 Deep Learning\nConcepts such as margins, regularization, and optimization directly influence modern neural network training and loss design.\n\n\n0.7.3 Bayesian and Statistical Learning\nAlthough SVMs are not probabilistic, their grounding in statistical learning theory provides a strong contrast to Bayesian approaches.\n\n\n0.7.4 Optimization Theory and Deployment\nKernel selection, solver efficiency, and scalability highlight real-world optimization challenges encountered in AI systems."
  },
  {
    "objectID": "index.html#introduction-1",
    "href": "index.html#introduction-1",
    "title": "Support Vector Machines Through the Four Lenses",
    "section": "0.8 Introduction",
    "text": "0.8 Introduction\nSupport Vector Machines (SVMs) are among the most powerful and mathematically elegant machine learning algorithms.\nBy examining SVM through the four foundational pillars‚ÄîLinear Algebra, Statistics, Calculus, and Optimization‚Äîwe uncover the deep mathematical structure that enables robust and generalizable learning.\nThis deconstruction demonstrates that even advanced AI systems are built upon a small set of fundamental mathematical principles."
  },
  {
    "objectID": "index.html#the-linear-algebra-perspective-1",
    "href": "index.html#the-linear-algebra-perspective-1",
    "title": "Support Vector Machines Through the Four Lenses",
    "section": "0.9 The Linear Algebra Perspective",
    "text": "0.9 The Linear Algebra Perspective\n\n0.9.1 Geometry of Separation\nFrom a linear algebra viewpoint, Support Vector Machines treat data as points in a high-dimensional vector space. Each input sample is represented as a feature vector, transforming learning into a problem of geometric separation.\nAt the core of SVM lies the concept of a hyperplane, defined by a weight vector and a bias term. This hyperplane acts as a decision boundary that separates data points belonging to different classes. Linear algebra governs how this boundary is oriented and positioned through operations such as dot products, vector norms, and projections.\nA key innovation of SVM is the margin‚Äîthe distance between the hyperplane and the nearest data points from each class. These nearest points, known as support vectors, uniquely define the model. As a result, SVM decisions depend on a small subset of critical data points rather than the entire dataset.\nWhen data is not linearly separable, SVMs employ kernel functions that implicitly map data into higher-dimensional spaces, where linear separation becomes feasible. This highlights the expressive power of linear algebra in modern machine learning."
  },
  {
    "objectID": "index.html#the-statistical-foundation-1",
    "href": "index.html#the-statistical-foundation-1",
    "title": "Support Vector Machines Through the Four Lenses",
    "section": "0.10 The Statistical Foundation",
    "text": "0.10 The Statistical Foundation\n\n0.10.1 Risk, Margin, and Generalization\nStatistics provides the framework for understanding what makes an SVM model effective. Unlike regression models that minimize squared error, SVMs rely on the hinge loss, which penalizes incorrect classifications and points that lie too close to the decision boundary.\nA central statistical concept in SVM is regularization, which balances two competing objectives:\n\nMaximizing the margin (model simplicity)\nMinimizing classification errors (data fit)\n\nThis balance reflects the classical bias‚Äìvariance trade-off. By controlling the regularization parameter, SVMs reduce overfitting while maintaining strong predictive performance.\nRather than modeling class probabilities explicitly, SVMs are grounded in statistical learning theory and the principle of structural risk minimization, ensuring robustness even in high-dimensional feature spaces."
  },
  {
    "objectID": "index.html#the-calculus-engine-1",
    "href": "index.html#the-calculus-engine-1",
    "title": "Support Vector Machines Through the Four Lenses",
    "section": "0.11 The Calculus Engine",
    "text": "0.11 The Calculus Engine\n\n0.11.1 Gradients and Constraints\nAlthough often presented geometrically, SVM training relies heavily on calculus. The learning objective is formulated as a constrained optimization problem, combining margin maximization with penalty terms for misclassification.\nCalculus enables this formulation through:\n\nGradients, which describe how the objective function changes\nLagrange multipliers, which incorporate constraints directly into the optimization process\n\nThese tools transform intuitive geometric goals into a mathematically solvable problem. Even when closed-form solvers or quadratic programming techniques are used, calculus remains the underlying engine that guarantees correctness and convergence."
  },
  {
    "objectID": "index.html#the-optimization-strategy-1",
    "href": "index.html#the-optimization-strategy-1",
    "title": "Support Vector Machines Through the Four Lenses",
    "section": "0.12 The Optimization Strategy",
    "text": "0.12 The Optimization Strategy\n\n0.12.1 Finding the Maximum-Margin Solution\nOptimization unifies all previous pillars into a practical learning algorithm. The SVM objective function is convex, meaning it has a single global minimum‚Äîan essential property for stable and reliable learning.\nOptimization theory governs:\n\nHow constraints are enforced\nWhy convergence is guaranteed\nHow efficiently solutions are found\n\nModern SVM implementations rely on quadratic programming and dual optimization methods, enabling efficient training even for large-scale datasets. The final model is defined entirely by its support vectors, making SVM both computationally efficient and interpretable."
  },
  {
    "objectID": "index.html#mapping-svm-to-the-four-pillars-1",
    "href": "index.html#mapping-svm-to-the-four-pillars-1",
    "title": "Support Vector Machines Through the Four Lenses",
    "section": "0.13 Mapping SVM to the Four Pillars",
    "text": "0.13 Mapping SVM to the Four Pillars\nSupport Vector Machines demonstrate a seamless integration of all four foundational pillars:\n\nLinear Algebra: Feature spaces, hyperplanes, margins, kernels\n\nStatistics: Generalization, regularization, risk minimization\n\nCalculus: Gradients, constrained optimization, Lagrangians\n\nOptimization: Convex solvers, convergence, efficiency\n\nThis unified view reveals SVM as a mathematically disciplined system rather than a black-box classifier."
  },
  {
    "objectID": "index.html#curriculum-alignment-1",
    "href": "index.html#curriculum-alignment-1",
    "title": "Support Vector Machines Through the Four Lenses",
    "section": "0.14 Curriculum Alignment",
    "text": "0.14 Curriculum Alignment\n\n0.14.1 Core Machine Learning\nSupport Vector Machines act as a bridge between simple linear models and advanced classifiers, drawing equally from all four pillars.\n\n\n0.14.2 Deep Learning\nConcepts such as margins, regularization, and optimization directly influence modern neural network training and loss design.\n\n\n0.14.3 Bayesian and Statistical Learning\nAlthough SVMs are not probabilistic, their grounding in statistical learning theory provides a strong contrast to Bayesian approaches.\n\n\n0.14.4 Optimization Theory and Deployment\nKernel selection, solver efficiency, and scalability highlight real-world optimization challenges encountered in AI systems."
  },
  {
    "objectID": "index.html#key-takeaway",
    "href": "index.html#key-takeaway",
    "title": "Support Vector Machines Through the Four Lenses",
    "section": "0.15 Key Takeaway",
    "text": "0.15 Key Takeaway\nSupport Vector Machines demonstrate that intelligence emerges from the disciplined interaction of mathematics.\nBy analyzing SVM through the four lenses, we uncover the same foundational structure that underpins both classical machine learning algorithms and modern AI systems."
  },
  {
    "objectID": "index.html#key-takeaway-1",
    "href": "index.html#key-takeaway-1",
    "title": "Support Vector Machines Through the Four Lenses",
    "section": "0.16 Key Takeaway",
    "text": "0.16 Key Takeaway\nSupport Vector Machines demonstrate that intelligence emerges from the disciplined interaction of mathematics.\nBy analyzing SVM through the four lenses, we uncover the same foundational structure that underpins both classical machine learning algorithms and modern AI systems.\n\n # Setup\n\n\nHi\n\n\n\n\n10"
  },
  {
    "objectID": "index.html#idea",
    "href": "index.html#idea",
    "title": "Support Vector Machines Through the Four Lenses",
    "section": "1.1 Idea",
    "text": "1.1 Idea\nPredict a continuous value using a straight-line relationship."
  },
  {
    "objectID": "index.html#when-to-use",
    "href": "index.html#when-to-use",
    "title": "Support Vector Machines Through the Four Lenses",
    "section": "1.2 When to use",
    "text": "1.2 When to use\n\nHouse price, marks prediction, trend estimation\nBaseline model for regression"
  },
  {
    "objectID": "index.html#demo-synthetic",
    "href": "index.html#demo-synthetic",
    "title": "Support Vector Machines Through the Four Lenses",
    "section": "1.3 Demo (synthetic)",
    "text": "1.3 Demo (synthetic)\n\n\nMSE: 3.4522247956202796\nR2 : 0.942791028778986"
  },
  {
    "objectID": "index.html#idea-1",
    "href": "index.html#idea-1",
    "title": "Support Vector Machines Through the Four Lenses",
    "section": "2.1 Idea",
    "text": "2.1 Idea\nPredict class probability using a sigmoid; good baseline for binary/multiclass."
  },
  {
    "objectID": "index.html#when-to-use-1",
    "href": "index.html#when-to-use-1",
    "title": "Support Vector Machines Through the Four Lenses",
    "section": "2.2 When to use",
    "text": "2.2 When to use\n\nSpam vs not spam, disease yes/no\nHighly interpretable baseline"
  },
  {
    "objectID": "index.html#demo-breast-cancer-dataset",
    "href": "index.html#demo-breast-cancer-dataset",
    "title": "Support Vector Machines Through the Four Lenses",
    "section": "2.3 Demo (Breast Cancer dataset)",
    "text": "2.3 Demo (Breast Cancer dataset)\n\n\nAccuracy: 0.9824561403508771\nF1      : 0.9861111111111112\n[[41  1]\n [ 1 71]]"
  },
  {
    "objectID": "index.html#idea-2",
    "href": "index.html#idea-2",
    "title": "Support Vector Machines Through the Four Lenses",
    "section": "3.1 Idea",
    "text": "3.1 Idea\nA point is classified by the majority label among its nearest neighbors."
  },
  {
    "objectID": "index.html#when-to-use-2",
    "href": "index.html#when-to-use-2",
    "title": "Support Vector Machines Through the Four Lenses",
    "section": "3.2 When to use",
    "text": "3.2 When to use\n\nSmall/medium datasets\nNonlinear boundaries, simple intuition\n\n\n\nAccuracy: 0.956140350877193\n              precision    recall  f1-score   support\n\n   malignant       0.95      0.93      0.94        42\n      benign       0.96      0.97      0.97        72\n\n    accuracy                           0.96       114\n   macro avg       0.96      0.95      0.95       114\nweighted avg       0.96      0.96      0.96       114"
  },
  {
    "objectID": "index.html#idea-3",
    "href": "index.html#idea-3",
    "title": "Support Vector Machines Through the Four Lenses",
    "section": "4.1 Idea",
    "text": "4.1 Idea\nSplit data by feature thresholds to reduce impurity (Gini/Entropy)."
  },
  {
    "objectID": "index.html#when-to-use-3",
    "href": "index.html#when-to-use-3",
    "title": "Support Vector Machines Through the Four Lenses",
    "section": "4.2 When to use",
    "text": "4.2 When to use\n\nMixed feature importance explanations\nEasy visualization (but can overfit)\n\n\n\nAccuracy: 0.9385964912280702"
  },
  {
    "objectID": "index.html#idea-4",
    "href": "index.html#idea-4",
    "title": "Support Vector Machines Through the Four Lenses",
    "section": "5.1 Idea",
    "text": "5.1 Idea\nMany trees + bagging; reduces overfitting and improves stability."
  },
  {
    "objectID": "index.html#when-to-use-4",
    "href": "index.html#when-to-use-4",
    "title": "Support Vector Machines Through the Four Lenses",
    "section": "5.2 When to use",
    "text": "5.2 When to use\n\nStrong general-purpose model\nGood with tabular data\n\n\n\nAccuracy: 0.9473684210526315"
  },
  {
    "objectID": "index.html#idea-5",
    "href": "index.html#idea-5",
    "title": "Support Vector Machines Through the Four Lenses",
    "section": "6.1 Idea",
    "text": "6.1 Idea\nFind a maximum-margin boundary; kernels allow nonlinear separation."
  },
  {
    "objectID": "index.html#when-to-use-5",
    "href": "index.html#when-to-use-5",
    "title": "Support Vector Machines Through the Four Lenses",
    "section": "6.2 When to use",
    "text": "6.2 When to use\n\nMedium-sized data\nStrong classifier with scaling\n\n\n\nAccuracy: 0.9824561403508771"
  },
  {
    "objectID": "index.html#idea-6",
    "href": "index.html#idea-6",
    "title": "Support Vector Machines Through the Four Lenses",
    "section": "7.1 Idea",
    "text": "7.1 Idea\nProbability-based classifier assuming feature independence."
  },
  {
    "objectID": "index.html#when-to-use-6",
    "href": "index.html#when-to-use-6",
    "title": "Support Vector Machines Through the Four Lenses",
    "section": "7.2 When to use",
    "text": "7.2 When to use\n\nText classification, quick baselines\nVery fast\n\n\n\nAccuracy: 0.9385964912280702"
  },
  {
    "objectID": "index.html#idea-7",
    "href": "index.html#idea-7",
    "title": "Support Vector Machines Through the Four Lenses",
    "section": "8.1 Idea",
    "text": "8.1 Idea\nUnsupervised grouping by minimizing distance to cluster centers."
  },
  {
    "objectID": "index.html#when-to-use-7",
    "href": "index.html#when-to-use-7",
    "title": "Support Vector Machines Through the Four Lenses",
    "section": "8.2 When to use",
    "text": "8.2 When to use\n\nMarket segmentation, grouping patterns\nQuick exploratory clustering\n\n\n\nC:\\Users\\noors\\anaconda3\\envs\\quarto\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning:\n\nKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2."
  },
  {
    "objectID": "posts/post.html",
    "href": "posts/post.html",
    "title": "Another blogpost example",
    "section": "",
    "text": "Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum."
  },
  {
    "objectID": "svm.html",
    "href": "svm.html",
    "title": "SVM Through the Four Lenses",
    "section": "",
    "text": "Author: Noorjahan\nDomain: Machine Learning Foundations\nAlgorithm: Support Vector Machines (SVM)\n\n\nüìå Objective\nThis notebook provides a conceptual and practical walkthrough of Support Vector Machines, integrating mathematical intuition with hands-on Python implementation.\n\n\n\nSupport Vector Machines (SVMs) are supervised learning algorithms used for classification and regression.\nThey are built on strong mathematical principles and provide excellent generalization performance.\nIn this notebook, we cover: - Mathematical intuition behind SVMs - Linear and kernel-based SVM models - Python implementation using scikit-learn - Visualization of decision boundaries## üìñ Introduction\nSupport Vector Machines (SVMs) are powerful supervised learning algorithms widely used for classification and regression tasks.\nThey are particularly valued for their strong theoretical guarantees and excellent generalization ability.\n\n\n\n\nüßÆ Mathematical intuition behind SVMs\n\nüßë‚Äçüíª Step-by-step Python implementation\n\nüìä Visualization of decision boundaries\n\nüß† Interpretation of model behavior\n\n\n\n\n\n\nThis is how a computer sees data. To a machine, patients, images, or lab results aren‚Äôt words or pictures ‚Äî they‚Äôre organized numbers arranged in tables and vectors. Linear algebra gives structure to this data and allows the model to combine different features, like age, symptoms, or gene counts, in a meaningful way.\nFrom a linear algebra standpoint, SVMs interpret data geometrically. Each input sample is represented as a vector in a high-dimensional feature space, and the entire dataset forms a matrix where rows correspond to samples and columns correspond to features.\nThe central goal of an SVM is to find a hyperplane that separates data points belonging to different classes. This hyperplane is defined by a weight vector and a bias term. Classification is achieved by computing the dot product between the input vectors and the weight vector, followed by an offset. This operation determines on which side of the hyperplane a data point lies.\nIn the case of non-linearly separable data, SVMs rely on kernel functions, which implicitly map data into higher-dimensional spaces where linear separation becomes possible. These kernel-induced transformations are purely linear algebraic operations, demonstrating how linear algebra enables powerful feature transformations without explicitly computing them.\nEach data point is represented as a vector in a high-dimensional space.\nSVM aims to find a hyperplane:\nw·µÄx + b = 0\nthat best separates data points from different classes.\n\n\n\n\nstatistics helps the model decide what matters and what doesn‚Äôt. Real-world data is noisy ‚Äî not every pattern is real. Statistics gives us tools to measure error, uncertainty, and confidence, and helps ensure the model doesn‚Äôt just memorize data, but actually learns patterns that generalize to new cases.\nStatistics provides the rationale for why SVMs generalize well to unseen data. Instead of merely minimizing classification error on the training set, SVMs aim to maximize the margin‚Äîthe distance between the separating hyperplane and the closest data points from each class (support vectors).\nThis margin maximization embodies a statistical principle: models with larger margins tend to have lower generalization error. By focusing only on a subset of critical points (the support vectors), SVMs reduce sensitivity to noise and outliers.\nIn soft-margin SVMs, a regularization parameter balances the trade-off between maximizing the margin and allowing misclassifications. This reflects a statistical bias‚Äìvariance trade-off, ensuring robustness when data are noisy or overlapping.\n\n\n\n\nCalculus is how the model learns. After making a prediction, the model asks: ‚ÄúHow wrong was I?‚Äù Calculus tells it which direction to move to improve. By making many small, smart adjustments, the model gradually gets better ‚Äî just like learning by correcting mistakes.\nAlthough SVMs are often solved using convex optimization techniques, calculus plays a crucial role in understanding their learning dynamics. The objective function in SVMs consists of a regularization term and a loss function‚Äîcommonly the hinge loss.\nThe gradient (or subgradient) of this loss function with respect to the model parameters indicates how changes in the parameters affect classification performance. These gradient-based insights allow iterative methods to update parameters efficiently.\nIn large-scale or online learning scenarios, gradient-based solvers such as stochastic gradient descent rely directly on calculus to adjust model parameters, highlighting how differentiation enables systematic learning rather than heuristic tuning.\nSVM uses the hinge loss:\nL(y, f(x)) = max(0, 1 ‚àí y f(x))\nGradients (or subgradients) of this loss guide parameter updates.\n\n\n\n\nOptimization is the strategy behind learning. It controls how fast the model learns, when it should stop, and how to reach the best solution efficiently. Without optimization, learning would be slow, unstable, or unreliable.\nOptimization theory provides the framework that makes SVMs both efficient and reliable. The SVM training problem is formulated as a convex optimization problem, meaning it has a single global optimum and no local minima.\nThis convexity guarantees that optimization algorithms converge to the best possible solution. Depending on the formulation, SVMs can be solved using quadratic programming, dual optimization, or gradient-based methods.\nOptimization principles guide critical design choices, such as selecting regularization parameters, choosing appropriate kernels, and determining convergence criteria. These choices ensure that training is computationally efficient while maintaining high predictive performance.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score, classification_report\n\nThis cell imports all the required Python libraries: - NumPy for numerical computations - Matplotlib for visualization - scikit-learn for dataset loading, model building, and evaluation\nSVC is the Support Vector Classifier used to implement SVM models.\n\n# Load Iris dataset\niris = datasets.load_iris()\nX = iris.data[:, :2]  # Select first two features for visualization\ny = iris.target\n\n# Convert to binary classification (class 0 vs class 1)\nX = X[y != 2]\ny = y[y != 2]\n\n# Split data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=42\n)\n\nIn this step: - The Iris dataset is loaded as a sample classification problem. - Only two features are selected to allow easy visualization of decision boundaries. - The problem is converted into binary classification. - The dataset is split into training and testing sets to evaluate generalization performance.\n\n\n\n\nWe begin with a linear kernel, which attempts to separate the classes using a straight-line decision boundary.\n\nsvm_linear = SVC(kernel='linear', C=1.0)\nsvm_linear.fit(X_train, y_train)\n\nSVC(kernel='linear')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.SVC?Documentation for SVCiFitted\n        \n            \n                Parameters\n                \n\n\n\n\nC¬†\n1.0\n\n\n\nkernel¬†\n'linear'\n\n\n\ndegree¬†\n3\n\n\n\ngamma¬†\n'scale'\n\n\n\ncoef0¬†\n0.0\n\n\n\nshrinking¬†\nTrue\n\n\n\nprobability¬†\nFalse\n\n\n\ntol¬†\n0.001\n\n\n\ncache_size¬†\n200\n\n\n\nclass_weight¬†\nNone\n\n\n\nverbose¬†\nFalse\n\n\n\nmax_iter¬†\n-1\n\n\n\ndecision_function_shape¬†\n'ovr'\n\n\n\nbreak_ties¬†\nFalse\n\n\n\nrandom_state¬†\nNone\n\n\n\n\n            \n        \n    \n\n\n\n\n\nA linear SVM attempts to separate data using a straight-line (or hyperplane) decision boundary.\n\n\n\nKernel: Linear\n\nC: Regularization parameter controlling margin vs misclassification\n\nWe begin with this simplest form of SVM to build intuition.\n\ny_pred = svm_linear.predict(X_test)\n\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred))\nprint(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n\nAccuracy: 1.0\n\nClassification Report:\n               precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        17\n           1       1.00      1.00      1.00        13\n\n    accuracy                           1.00        30\n   macro avg       1.00      1.00      1.00        30\nweighted avg       1.00      1.00      1.00        30\n\n\n\n\n\n\nThe trained model is evaluated using: - Accuracy ‚Äì overall correctness - Precision & Recall ‚Äì class-wise performance - F1-score ‚Äì balance between precision and recall\n\nüéØ Evaluation on unseen data helps assess real-world performance.\n\n\n\n\n\nVisualization provides geometric intuition into SVM behavior.\n\n\n\nThe decision boundary (hyperplane)\nThe margin\nDistribution of training samples\n\n\nüìê Only a few critical points ‚Äî support vectors ‚Äî define the boundary.\n\n\ndef plot_decision_boundary(model, X, y):\n    h = 0.02\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(\n        np.arange(x_min, x_max, h),\n        np.arange(y_min, y_max, h)\n    )\n    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n\n    plt.contourf(xx, yy, Z, alpha=0.3)\n    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolor='k')\n    plt.xlabel(\"Feature 1\")\n    plt.ylabel(\"Feature 2\")\n    plt.title(\"SVM Decision Boundary\")\n    plt.show()\n\nplot_decision_boundary(svm_linear, X_train, y_train)\n\n\n\n\n\n\n\n\nThis function: - Creates a grid over the feature space - Predicts class labels for each grid point - Visualizes the decision boundary and training samples\nThis demonstrates how SVM constructs a separating hyperplane based on support vectors.\n\nsvm_rbf = SVC(kernel='rbf', C=1.0, gamma='scale')\nsvm_rbf.fit(X_train, y_train)\n\ny_pred_rbf = svm_rbf.predict(X_test)\nprint(\"RBF Kernel Accuracy:\", accuracy_score(y_test, y_pred_rbf))\n\nRBF Kernel Accuracy: 1.0\n\n\n\n\n\n\nReal-world data is often not linearly separable.\nKernel functions enable SVMs to model non-linear decision boundaries.\n\n\n\nImplicitly maps data to higher dimensions\nCaptures complex patterns\nControlled by the gamma parameter\n\nThis allows SVMs to remain powerful even in complex feature spaces.\n\n\n\n\n‚úî Linear SVM performs well for linearly separable data\n‚úî Kernel SVM captures non-linear structures\n‚úî Only support vectors influence the decision boundary\n‚úî Margin maximization improves robustness\n\n‚ú® SVM decisions are driven by geometry, not sheer data volume.\n\n\n\n\nSupport Vector Machines elegantly integrate:\n\nüßÆ Linear Algebra ‚Äî feature representation & hyperplanes\n\nüìä Statistics ‚Äî margin-based generalization\n\nüìê Calculus ‚Äî loss functions & gradients\n\n‚öôÔ∏è Optimization ‚Äî convex learning guarantees\n\nSVMs remain a cornerstone of classical machine learning, combining strong theory with practical performance."
  },
  {
    "objectID": "svm.html#mathematical-foundations-of-svm",
    "href": "svm.html#mathematical-foundations-of-svm",
    "title": "SVM Through the Four Lenses",
    "section": "",
    "text": "This is how a computer sees data. To a machine, patients, images, or lab results aren‚Äôt words or pictures ‚Äî they‚Äôre organized numbers arranged in tables and vectors. Linear algebra gives structure to this data and allows the model to combine different features, like age, symptoms, or gene counts, in a meaningful way.\nFrom a linear algebra standpoint, SVMs interpret data geometrically. Each input sample is represented as a vector in a high-dimensional feature space, and the entire dataset forms a matrix where rows correspond to samples and columns correspond to features.\nThe central goal of an SVM is to find a hyperplane that separates data points belonging to different classes. This hyperplane is defined by a weight vector and a bias term. Classification is achieved by computing the dot product between the input vectors and the weight vector, followed by an offset. This operation determines on which side of the hyperplane a data point lies.\nIn the case of non-linearly separable data, SVMs rely on kernel functions, which implicitly map data into higher-dimensional spaces where linear separation becomes possible. These kernel-induced transformations are purely linear algebraic operations, demonstrating how linear algebra enables powerful feature transformations without explicitly computing them.\nEach data point is represented as a vector in a high-dimensional space.\nSVM aims to find a hyperplane:\nw·µÄx + b = 0\nthat best separates data points from different classes.\n\n\n\n\nstatistics helps the model decide what matters and what doesn‚Äôt. Real-world data is noisy ‚Äî not every pattern is real. Statistics gives us tools to measure error, uncertainty, and confidence, and helps ensure the model doesn‚Äôt just memorize data, but actually learns patterns that generalize to new cases.\nStatistics provides the rationale for why SVMs generalize well to unseen data. Instead of merely minimizing classification error on the training set, SVMs aim to maximize the margin‚Äîthe distance between the separating hyperplane and the closest data points from each class (support vectors).\nThis margin maximization embodies a statistical principle: models with larger margins tend to have lower generalization error. By focusing only on a subset of critical points (the support vectors), SVMs reduce sensitivity to noise and outliers.\nIn soft-margin SVMs, a regularization parameter balances the trade-off between maximizing the margin and allowing misclassifications. This reflects a statistical bias‚Äìvariance trade-off, ensuring robustness when data are noisy or overlapping.\n\n\n\n\nCalculus is how the model learns. After making a prediction, the model asks: ‚ÄúHow wrong was I?‚Äù Calculus tells it which direction to move to improve. By making many small, smart adjustments, the model gradually gets better ‚Äî just like learning by correcting mistakes.\nAlthough SVMs are often solved using convex optimization techniques, calculus plays a crucial role in understanding their learning dynamics. The objective function in SVMs consists of a regularization term and a loss function‚Äîcommonly the hinge loss.\nThe gradient (or subgradient) of this loss function with respect to the model parameters indicates how changes in the parameters affect classification performance. These gradient-based insights allow iterative methods to update parameters efficiently.\nIn large-scale or online learning scenarios, gradient-based solvers such as stochastic gradient descent rely directly on calculus to adjust model parameters, highlighting how differentiation enables systematic learning rather than heuristic tuning.\nSVM uses the hinge loss:\nL(y, f(x)) = max(0, 1 ‚àí y f(x))\nGradients (or subgradients) of this loss guide parameter updates.\n\n\n\n\nOptimization is the strategy behind learning. It controls how fast the model learns, when it should stop, and how to reach the best solution efficiently. Without optimization, learning would be slow, unstable, or unreliable.\nOptimization theory provides the framework that makes SVMs both efficient and reliable. The SVM training problem is formulated as a convex optimization problem, meaning it has a single global optimum and no local minima.\nThis convexity guarantees that optimization algorithms converge to the best possible solution. Depending on the formulation, SVMs can be solved using quadratic programming, dual optimization, or gradient-based methods.\nOptimization principles guide critical design choices, such as selecting regularization parameters, choosing appropriate kernels, and determining convergence criteria. These choices ensure that training is computationally efficient while maintaining high predictive performance.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score, classification_report\n\nThis cell imports all the required Python libraries: - NumPy for numerical computations - Matplotlib for visualization - scikit-learn for dataset loading, model building, and evaluation\nSVC is the Support Vector Classifier used to implement SVM models.\n\n# Load Iris dataset\niris = datasets.load_iris()\nX = iris.data[:, :2]  # Select first two features for visualization\ny = iris.target\n\n# Convert to binary classification (class 0 vs class 1)\nX = X[y != 2]\ny = y[y != 2]\n\n# Split data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=42\n)\n\nIn this step: - The Iris dataset is loaded as a sample classification problem. - Only two features are selected to allow easy visualization of decision boundaries. - The problem is converted into binary classification. - The dataset is split into training and testing sets to evaluate generalization performance."
  },
  {
    "objectID": "svm.html#training-a-linear-svm",
    "href": "svm.html#training-a-linear-svm",
    "title": "SVM Through the Four Lenses",
    "section": "",
    "text": "We begin with a linear kernel, which attempts to separate the classes using a straight-line decision boundary.\n\nsvm_linear = SVC(kernel='linear', C=1.0)\nsvm_linear.fit(X_train, y_train)\n\nSVC(kernel='linear')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.SVC?Documentation for SVCiFitted\n        \n            \n                Parameters\n                \n\n\n\n\nC¬†\n1.0\n\n\n\nkernel¬†\n'linear'\n\n\n\ndegree¬†\n3\n\n\n\ngamma¬†\n'scale'\n\n\n\ncoef0¬†\n0.0\n\n\n\nshrinking¬†\nTrue\n\n\n\nprobability¬†\nFalse\n\n\n\ntol¬†\n0.001\n\n\n\ncache_size¬†\n200\n\n\n\nclass_weight¬†\nNone\n\n\n\nverbose¬†\nFalse\n\n\n\nmax_iter¬†\n-1\n\n\n\ndecision_function_shape¬†\n'ovr'\n\n\n\nbreak_ties¬†\nFalse\n\n\n\nrandom_state¬†\nNone"
  },
  {
    "objectID": "svm.html#training-a-linear-svm-1",
    "href": "svm.html#training-a-linear-svm-1",
    "title": "SVM Through the Four Lenses",
    "section": "",
    "text": "A linear SVM attempts to separate data using a straight-line (or hyperplane) decision boundary.\n\n\n\nKernel: Linear\n\nC: Regularization parameter controlling margin vs misclassification\n\nWe begin with this simplest form of SVM to build intuition.\n\ny_pred = svm_linear.predict(X_test)\n\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred))\nprint(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n\nAccuracy: 1.0\n\nClassification Report:\n               precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        17\n           1       1.00      1.00      1.00        13\n\n    accuracy                           1.00        30\n   macro avg       1.00      1.00      1.00        30\nweighted avg       1.00      1.00      1.00        30\n\n\n\n\n\n\nThe trained model is evaluated using: - Accuracy ‚Äì overall correctness - Precision & Recall ‚Äì class-wise performance - F1-score ‚Äì balance between precision and recall\n\nüéØ Evaluation on unseen data helps assess real-world performance."
  },
  {
    "objectID": "svm.html#visualizing-the-decision-boundary",
    "href": "svm.html#visualizing-the-decision-boundary",
    "title": "SVM Through the Four Lenses",
    "section": "",
    "text": "Visualization provides geometric intuition into SVM behavior.\n\n\n\nThe decision boundary (hyperplane)\nThe margin\nDistribution of training samples\n\n\nüìê Only a few critical points ‚Äî support vectors ‚Äî define the boundary.\n\n\ndef plot_decision_boundary(model, X, y):\n    h = 0.02\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(\n        np.arange(x_min, x_max, h),\n        np.arange(y_min, y_max, h)\n    )\n    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n\n    plt.contourf(xx, yy, Z, alpha=0.3)\n    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolor='k')\n    plt.xlabel(\"Feature 1\")\n    plt.ylabel(\"Feature 2\")\n    plt.title(\"SVM Decision Boundary\")\n    plt.show()\n\nplot_decision_boundary(svm_linear, X_train, y_train)\n\n\n\n\n\n\n\n\nThis function: - Creates a grid over the feature space - Predicts class labels for each grid point - Visualizes the decision boundary and training samples\nThis demonstrates how SVM constructs a separating hyperplane based on support vectors.\n\nsvm_rbf = SVC(kernel='rbf', C=1.0, gamma='scale')\nsvm_rbf.fit(X_train, y_train)\n\ny_pred_rbf = svm_rbf.predict(X_test)\nprint(\"RBF Kernel Accuracy:\", accuracy_score(y_test, y_pred_rbf))\n\nRBF Kernel Accuracy: 1.0"
  },
  {
    "objectID": "svm.html#kernel-svm-rbf-kernel",
    "href": "svm.html#kernel-svm-rbf-kernel",
    "title": "SVM Through the Four Lenses",
    "section": "",
    "text": "Real-world data is often not linearly separable.\nKernel functions enable SVMs to model non-linear decision boundaries.\n\n\n\nImplicitly maps data to higher dimensions\nCaptures complex patterns\nControlled by the gamma parameter\n\nThis allows SVMs to remain powerful even in complex feature spaces."
  },
  {
    "objectID": "svm.html#interpretation-of-results",
    "href": "svm.html#interpretation-of-results",
    "title": "SVM Through the Four Lenses",
    "section": "",
    "text": "‚úî Linear SVM performs well for linearly separable data\n‚úî Kernel SVM captures non-linear structures\n‚úî Only support vectors influence the decision boundary\n‚úî Margin maximization improves robustness\n\n‚ú® SVM decisions are driven by geometry, not sheer data volume."
  },
  {
    "objectID": "svm.html#conclusion",
    "href": "svm.html#conclusion",
    "title": "SVM Through the Four Lenses",
    "section": "",
    "text": "Support Vector Machines elegantly integrate:\n\nüßÆ Linear Algebra ‚Äî feature representation & hyperplanes\n\nüìä Statistics ‚Äî margin-based generalization\n\nüìê Calculus ‚Äî loss functions & gradients\n\n‚öôÔ∏è Optimization ‚Äî convex learning guarantees\n\nSVMs remain a cornerstone of classical machine learning, combining strong theory with practical performance."
  }
]