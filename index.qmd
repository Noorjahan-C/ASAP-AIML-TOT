---
title: "My Personal Webpage"
---



![alt](images/user_image.png)
# Setup
```{python}
print("Hi")
```
```{python} 
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report

x=10
print(x)
```

# 1) Linear Regression

## Idea

Predict a continuous value using a straight-line relationship.

## When to use

* House price, marks prediction, trend estimation
* Baseline model for regression

## Demo (synthetic)

```{python}
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# Data

rng = np.random.default_rng(7)
X = rng.uniform(0, 10, size=(120, 1))
y = 3.5*X.squeeze() + 8 + rng.normal(0, 2.0, size=120)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model = LinearRegression()
model.fit(X_train, y_train)
pred = model.predict(X_test)

print("MSE:", mean_squared_error(y_test, pred))
print("R2 :", r2_score(y_test, pred))

# Plot

plt.scatter(X_test, y_test)
plt.scatter(X_test, pred)
plt.xlabel("X"); plt.ylabel("y")
plt.title("Linear Regression: true vs predicted")
plt.show()
```


# 2) Logistic Regression (Classification)

## Idea

Predict class probability using a sigmoid; good baseline for binary/multiclass.

## When to use

* Spam vs not spam, disease yes/no
* Highly interpretable baseline

## Demo (Breast Cancer dataset)

```{python}
from sklearn.datasets import load_breast_cancer
from sklearn.linear_model import LogisticRegression

data = load_breast_cancer()
X, y = data.data, data.target

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)

clf = Pipeline([
("scaler", StandardScaler()),
("lr", LogisticRegression(max_iter=5000))
])

clf.fit(X_train, y_train)
pred = clf.predict(X_test)

print("Accuracy:", accuracy_score(y_test, pred))
print("F1      :", f1_score(y_test, pred))
print(confusion_matrix(y_test, pred))
```


# 3) k-NN

## Idea

A point is classified by the majority label among its nearest neighbors.

## When to use

* Small/medium datasets
* Nonlinear boundaries, simple intuition

```{python}
from sklearn.neighbors import KNeighborsClassifier

knn = Pipeline([
("scaler", StandardScaler()),
("knn", KNeighborsClassifier(n_neighbors=5))
])

knn.fit(X_train, y_train)
pred = knn.predict(X_test)

print("Accuracy:", accuracy_score(y_test, pred))
print(classification_report(y_test, pred, target_names=data.target_names))
```



# 4) Decision Tree

## Idea

Split data by feature thresholds to reduce impurity (Gini/Entropy).

## When to use

* Mixed feature importance explanations
* Easy visualization (but can overfit)

```{python}
from sklearn.tree import DecisionTreeClassifier

tree = DecisionTreeClassifier(max_depth=4, random_state=42)
tree.fit(X_train, y_train)
pred = tree.predict(X_test)

print("Accuracy:", accuracy_score(y_test, pred))
```


# 5) Random Forest

## Idea

Many trees + bagging; reduces overfitting and improves stability.

## When to use

* Strong general-purpose model
* Good with tabular data

```{python}
from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier(
n_estimators=300,
max_depth=None,
random_state=42,
n_jobs=-1
)

rf.fit(X_train, y_train)
pred = rf.predict(X_test)

print("Accuracy:", accuracy_score(y_test, pred))
```



# 6) Support Vector Machine (SVM)

## Idea

Find a maximum-margin boundary; kernels allow nonlinear separation.

## When to use

* Medium-sized data
* Strong classifier with scaling

```{python}
from sklearn.svm import SVC

svm = Pipeline([
("scaler", StandardScaler()),
("svc", SVC(kernel="rbf", C=2.0, gamma="scale"))
])

svm.fit(X_train, y_train)
pred = svm.predict(X_test)

print("Accuracy:", accuracy_score(y_test, pred))
```


# 7) Naive Bayes

## Idea

Probability-based classifier assuming feature independence.

## When to use

* Text classification, quick baselines
* Very fast

```{python}
from sklearn.naive_bayes import GaussianNB

nb = GaussianNB()
nb.fit(X_train, y_train)
pred = nb.predict(X_test)

print("Accuracy:", accuracy_score(y_test, pred))
```



# 8) k-Means (Clustering)

## Idea

Unsupervised grouping by minimizing distance to cluster centers.

## When to use

* Market segmentation, grouping patterns
* Quick exploratory clustering

```{python}
from sklearn.datasets import make_blobs
from sklearn.cluster import KMeans

Xc, _ = make_blobs(n_samples=300, centers=4, random_state=42, cluster_std=1.2)

kmeans = KMeans(n_clusters=4, n_init=10, random_state=42)
labels = kmeans.fit_predict(Xc)

plt.scatter(Xc[:,0], Xc[:,1], c=labels)
plt.title("k-Means clustering")
plt.show()
```


# Quick Comparison Table

| Task           | Good baseline       | Strong tabular          | Interpretable     | Handles nonlinearity |
| -------------- | ------------------- | ----------------------- | ----------------- | -------------------- |
| Regression     | Linear Regression   | Random Forest Regressor | Linear Regression | RF / SVR             |
| Classification | Logistic Regression | Random Forest           | LR / Tree         | SVM / RF / kNN       |
| Clustering     | k-Means             | â€”                       | Medium            | Depends              |

---

# Homework Ideas

1. Train **Logistic Regression + Random Forest** and compare **F1**.
2. Add **hyperparameter tuning** (GridSearchCV) for SVM or RF.
3. Plot **feature importance** for Random Forest.


