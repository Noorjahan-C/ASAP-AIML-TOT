---
title: "My Personal Webpage"
---
---
title: "Support Vector Machines Through the Four Lenses"
subtitle: "Deconstructing Intelligence Using Mathematical Foundations"
author: "Noorjahan Chorath"
format:
  html:
    toc: true
    toc-depth: 3
    number-sections: true
    theme: cosmo
execute:
  echo: false
---

## Introduction

Support Vector Machines (SVMs) are among the most powerful and mathematically elegant machine learning algorithms.  
By examining SVM through the four foundational pillars—**Linear Algebra, Statistics, Calculus, and Optimization**—we uncover the deep mathematical structure that enables robust and generalizable learning.

This deconstruction demonstrates that even advanced AI systems are built upon a small set of fundamental mathematical principles.

---

## The Linear Algebra Perspective  
### *Geometry of Separation*

From a linear algebra viewpoint, Support Vector Machines treat data as **points in a high-dimensional vector space**. Each input sample is represented as a feature vector, transforming learning into a problem of **geometric separation**.

At the core of SVM lies the concept of a **hyperplane**, defined by a weight vector and a bias term. This hyperplane acts as a decision boundary that separates data points belonging to different classes. Linear algebra governs how this boundary is oriented and positioned through operations such as dot products, vector norms, and projections.

A key innovation of SVM is the **margin**—the distance between the hyperplane and the nearest data points from each class. These nearest points, known as **support vectors**, uniquely define the model. As a result, SVM decisions depend on a small subset of critical data points rather than the entire dataset.

When data is not linearly separable, SVMs employ **kernel functions** that implicitly map data into higher-dimensional spaces, where linear separation becomes feasible. This highlights the expressive power of linear algebra in modern machine learning.

---

## The Statistical Foundation  
### *Risk, Margin, and Generalization*

Statistics provides the framework for understanding what makes an SVM model effective. Unlike regression models that minimize squared error, SVMs rely on the **hinge loss**, which penalizes incorrect classifications and points that lie too close to the decision boundary.

A central statistical concept in SVM is **regularization**, which balances two competing objectives:

- Maximizing the margin (model simplicity)
- Minimizing classification errors (data fit)

This balance reflects the classical **bias–variance trade-off**. By controlling the regularization parameter, SVMs reduce overfitting while maintaining strong predictive performance.

Rather than modeling class probabilities explicitly, SVMs are grounded in **statistical learning theory** and the principle of **structural risk minimization**, ensuring robustness even in high-dimensional feature spaces.

---

## The Calculus Engine  
### *Gradients and Constraints*

Although often presented geometrically, SVM training relies heavily on calculus. The learning objective is formulated as a constrained optimization problem, combining margin maximization with penalty terms for misclassification.

Calculus enables this formulation through:

- **Gradients**, which describe how the objective function changes
- **Lagrange multipliers**, which incorporate constraints directly into the optimization process

These tools transform intuitive geometric goals into a mathematically solvable problem. Even when closed-form solvers or quadratic programming techniques are used, calculus remains the underlying engine that guarantees correctness and convergence.

---

## The Optimization Strategy  
### *Finding the Maximum-Margin Solution*

Optimization unifies all previous pillars into a practical learning algorithm. The SVM objective function is **convex**, meaning it has a single global minimum—an essential property for stable and reliable learning.

Optimization theory governs:

- How constraints are enforced
- Why convergence is guaranteed
- How efficiently solutions are found

Modern SVM implementations rely on **quadratic programming** and **dual optimization** methods, enabling efficient training even for large-scale datasets. The final model is defined entirely by its support vectors, making SVM both computationally efficient and interpretable.

---

## Mapping SVM to the Four Pillars

Support Vector Machines demonstrate a seamless integration of all four foundational pillars:

- **Linear Algebra**: Feature spaces, hyperplanes, margins, kernels  
- **Statistics**: Generalization, regularization, risk minimization  
- **Calculus**: Gradients, constrained optimization, Lagrangians  
- **Optimization**: Convex solvers, convergence, efficiency  

This unified view reveals SVM as a mathematically disciplined system rather than a black-box classifier.

---

## Curriculum Alignment

### Core Machine Learning
Support Vector Machines act as a bridge between simple linear models and advanced classifiers, drawing equally from all four pillars.

### Deep Learning
Concepts such as margins, regularization, and optimization directly influence modern neural network training and loss design.

### Bayesian and Statistical Learning
Although SVMs are not probabilistic, their grounding in statistical learning theory provides a strong contrast to Bayesian approaches.

### Optimization Theory and Deployment
Kernel selection, solver efficiency, and scalability highlight real-world optimization challenges encountered in AI systems.


## Introduction

Support Vector Machines (SVMs) are among the most powerful and mathematically elegant machine learning algorithms.  
By examining SVM through the four foundational pillars—**Linear Algebra, Statistics, Calculus, and Optimization**—we uncover the deep mathematical structure that enables robust and generalizable learning.

This deconstruction demonstrates that even advanced AI systems are built upon a small set of fundamental mathematical principles.

---

## The Linear Algebra Perspective  
### *Geometry of Separation*

From a linear algebra viewpoint, Support Vector Machines treat data as **points in a high-dimensional vector space**. Each input sample is represented as a feature vector, transforming learning into a problem of **geometric separation**.

At the core of SVM lies the concept of a **hyperplane**, defined by a weight vector and a bias term. This hyperplane acts as a decision boundary that separates data points belonging to different classes. Linear algebra governs how this boundary is oriented and positioned through operations such as dot products, vector norms, and projections.

A key innovation of SVM is the **margin**—the distance between the hyperplane and the nearest data points from each class. These nearest points, known as **support vectors**, uniquely define the model. As a result, SVM decisions depend on a small subset of critical data points rather than the entire dataset.

When data is not linearly separable, SVMs employ **kernel functions** that implicitly map data into higher-dimensional spaces, where linear separation becomes feasible. This highlights the expressive power of linear algebra in modern machine learning.

---

## The Statistical Foundation  
### *Risk, Margin, and Generalization*

Statistics provides the framework for understanding what makes an SVM model effective. Unlike regression models that minimize squared error, SVMs rely on the **hinge loss**, which penalizes incorrect classifications and points that lie too close to the decision boundary.

A central statistical concept in SVM is **regularization**, which balances two competing objectives:

- Maximizing the margin (model simplicity)
- Minimizing classification errors (data fit)

This balance reflects the classical **bias–variance trade-off**. By controlling the regularization parameter, SVMs reduce overfitting while maintaining strong predictive performance.

Rather than modeling class probabilities explicitly, SVMs are grounded in **statistical learning theory** and the principle of **structural risk minimization**, ensuring robustness even in high-dimensional feature spaces.

---

## The Calculus Engine  
### *Gradients and Constraints*

Although often presented geometrically, SVM training relies heavily on calculus. The learning objective is formulated as a constrained optimization problem, combining margin maximization with penalty terms for misclassification.

Calculus enables this formulation through:

- **Gradients**, which describe how the objective function changes
- **Lagrange multipliers**, which incorporate constraints directly into the optimization process

These tools transform intuitive geometric goals into a mathematically solvable problem. Even when closed-form solvers or quadratic programming techniques are used, calculus remains the underlying engine that guarantees correctness and convergence.

---

## The Optimization Strategy  
### *Finding the Maximum-Margin Solution*

Optimization unifies all previous pillars into a practical learning algorithm. The SVM objective function is **convex**, meaning it has a single global minimum—an essential property for stable and reliable learning.

Optimization theory governs:

- How constraints are enforced
- Why convergence is guaranteed
- How efficiently solutions are found

Modern SVM implementations rely on **quadratic programming** and **dual optimization** methods, enabling efficient training even for large-scale datasets. The final model is defined entirely by its support vectors, making SVM both computationally efficient and interpretable.

---

## Mapping SVM to the Four Pillars

Support Vector Machines demonstrate a seamless integration of all four foundational pillars:

- **Linear Algebra**: Feature spaces, hyperplanes, margins, kernels  
- **Statistics**: Generalization, regularization, risk minimization  
- **Calculus**: Gradients, constrained optimization, Lagrangians  
- **Optimization**: Convex solvers, convergence, efficiency  

This unified view reveals SVM as a mathematically disciplined system rather than a black-box classifier.

---

## Curriculum Alignment

### Core Machine Learning
Support Vector Machines act as a bridge between simple linear models and advanced classifiers, drawing equally from all four pillars.

### Deep Learning
Concepts such as margins, regularization, and optimization directly influence modern neural network training and loss design.

### Bayesian and Statistical Learning
Although SVMs are not probabilistic, their grounding in statistical learning theory provides a strong contrast to Bayesian approaches.

### Optimization Theory and Deployment
Kernel selection, solver efficiency, and scalability highlight real-world optimization challenges encountered in AI systems.

---

## Key Takeaway

Support Vector Machines demonstrate that **intelligence emerges from the disciplined interaction of mathematics**.  
By analyzing SVM through the four lenses, we uncover the same foundational structure that underpins both classical machine learning algorithms and modern AI systems.

---


## Key Takeaway

Support Vector Machines demonstrate that **intelligence emerges from the disciplined interaction of mathematics**.  
By analyzing SVM through the four lenses, we uncover the same foundational structure that underpins both classical machine learning algorithms and modern AI systems.

---




![alt](images/user_image.png)
# Setup
```{python}
print("Hi")
```
```{python} 
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report

x=10
print(x)
```

# 1) Linear Regression

## Idea

Predict a continuous value using a straight-line relationship.

## When to use

* House price, marks prediction, trend estimation
* Baseline model for regression

## Demo (synthetic)

```{python}
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# Data

rng = np.random.default_rng(7)
X = rng.uniform(0, 10, size=(120, 1))
y = 3.5*X.squeeze() + 8 + rng.normal(0, 2.0, size=120)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model = LinearRegression()
model.fit(X_train, y_train)
pred = model.predict(X_test)

print("MSE:", mean_squared_error(y_test, pred))
print("R2 :", r2_score(y_test, pred))

# Plot

plt.scatter(X_test, y_test)
plt.scatter(X_test, pred)
plt.xlabel("X"); plt.ylabel("y")
plt.title("Linear Regression: true vs predicted")
plt.show()
```


# 2) Logistic Regression (Classification)

## Idea

Predict class probability using a sigmoid; good baseline for binary/multiclass.

## When to use

* Spam vs not spam, disease yes/no
* Highly interpretable baseline

## Demo (Breast Cancer dataset)

```{python}
from sklearn.datasets import load_breast_cancer
from sklearn.linear_model import LogisticRegression

data = load_breast_cancer()
X, y = data.data, data.target

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)

clf = Pipeline([
("scaler", StandardScaler()),
("lr", LogisticRegression(max_iter=5000))
])

clf.fit(X_train, y_train)
pred = clf.predict(X_test)

print("Accuracy:", accuracy_score(y_test, pred))
print("F1      :", f1_score(y_test, pred))
print(confusion_matrix(y_test, pred))
```


# 3) k-NN

## Idea

A point is classified by the majority label among its nearest neighbors.

## When to use

* Small/medium datasets
* Nonlinear boundaries, simple intuition

```{python}
from sklearn.neighbors import KNeighborsClassifier

knn = Pipeline([
("scaler", StandardScaler()),
("knn", KNeighborsClassifier(n_neighbors=5))
])

knn.fit(X_train, y_train)
pred = knn.predict(X_test)

print("Accuracy:", accuracy_score(y_test, pred))
print(classification_report(y_test, pred, target_names=data.target_names))
```



# 4) Decision Tree

## Idea

Split data by feature thresholds to reduce impurity (Gini/Entropy).

## When to use

* Mixed feature importance explanations
* Easy visualization (but can overfit)

```{python}
from sklearn.tree import DecisionTreeClassifier

tree = DecisionTreeClassifier(max_depth=4, random_state=42)
tree.fit(X_train, y_train)
pred = tree.predict(X_test)

print("Accuracy:", accuracy_score(y_test, pred))
```


# 5) Random Forest

## Idea

Many trees + bagging; reduces overfitting and improves stability.

## When to use

* Strong general-purpose model
* Good with tabular data

```{python}
from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier(
n_estimators=300,
max_depth=None,
random_state=42,
n_jobs=-1
)

rf.fit(X_train, y_train)
pred = rf.predict(X_test)

print("Accuracy:", accuracy_score(y_test, pred))
```



# 6) Support Vector Machine (SVM)

## Idea

Find a maximum-margin boundary; kernels allow nonlinear separation.

## When to use

* Medium-sized data
* Strong classifier with scaling

```{python}
from sklearn.svm import SVC

svm = Pipeline([
("scaler", StandardScaler()),
("svc", SVC(kernel="rbf", C=2.0, gamma="scale"))
])

svm.fit(X_train, y_train)
pred = svm.predict(X_test)

print("Accuracy:", accuracy_score(y_test, pred))
```


# 7) Naive Bayes

## Idea

Probability-based classifier assuming feature independence.

## When to use

* Text classification, quick baselines
* Very fast

```{python}
from sklearn.naive_bayes import GaussianNB

nb = GaussianNB()
nb.fit(X_train, y_train)
pred = nb.predict(X_test)

print("Accuracy:", accuracy_score(y_test, pred))
```



# 8) k-Means (Clustering)


## Idea

Unsupervised grouping by minimizing distance to cluster centers.

## When to use

* Market segmentation, grouping patterns
* Quick exploratory clustering

```{python}
from sklearn.datasets import make_blobs
from sklearn.cluster import KMeans

Xc, _ = make_blobs(n_samples=300, centers=4, random_state=42, cluster_std=1.2)

kmeans = KMeans(n_clusters=4, n_init=10, random_state=42)
labels = kmeans.fit_predict(Xc)

plt.scatter(Xc[:,0], Xc[:,1], c=labels)
plt.title("k-Means clustering")
plt.show()
```


# Quick Comparison Table

| Task           | Good baseline       | Strong tabular          | Interpretable     | Handles nonlinearity |
| -------------- | ------------------- | ----------------------- | ----------------- | -------------------- |
| Regression     | Linear Regression   | Random Forest Regressor | Linear Regression | RF / SVR             |
| Classification | Logistic Regression | Random Forest           | LR / Tree         | SVM / RF / kNN       |
| Clustering     | k-Means             | —                       | Medium            | Depends              |

---

# Homework Ideas

1. Train **Logistic Regression + Random Forest** and compare **F1**.
2. Add **hyperparameter tuning** (GridSearchCV) for SVM or RF.
3. Plot **feature importance** for Random Forest.


